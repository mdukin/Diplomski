Evo odgovora na pitanja:

1. Prednost algoritma AdaGrad nad osnovnom implementacijom SGD-a je:
(b) skaliranje komponenata gradijenata njihovom akumuliranom normom

2. Povećanjem veličine mini-grupe dobivamo:
(d) manji broj preciznijih ažuriranja parametara modela

3. Zašto potpuno povezani modeli s prosječnom unakrsnom entropijom imaju više jednako dobrih lokalnih minimuma?
(a) zbog simetrije neurona istog skrivenog sloja

4. Kako biste opisali odnos između gubitka „0-1“ i unakrsne entropije?
(c) unakrsna entropija je derivabilni nadomjestak gubitka „0-1“

5. Kod uporabe lokalne normalizacije odziva (LRN), normaliziraju se:
(c) izlazi neurona s obzirom na izlaze njemu susjednih neurona u istom sloju

6. Predtreniranje je postupak:
(c) koji osigurava početne vrijednosti parametara mreže koje su bolje od slučajnih

7. Do problema eksplodirajućeg gradijenta može doći ako se podatci uzastopno
 množe matricom težina za koju vrijedi:
(b) da ima neke svojstvene vrijednosti koje su po iznosu dosta veće od 1

8. Zašto pri optimizaciji radije promatramo log-izglednost nego izglednost?
(c) zbog aditivnosti gradijenata nezavisnih podataka

9. Uzorkovanje manjeg broja uzoraka (mini-grupe) umjesto uporabe
čitavog skupa uzoraka za učenje pri izračunu gradijenata je opravdano jer:
(a) preciznost određivanja gradijenta s povećanjem broja uzoraka raste ispodlinearno

10. U kojoj od sljedećih situacija nam ne može pomoći učenje s momentom?
(d) kod „ploha“

11. Kod normalizacije nad grupom normalizirani se podatci na kraju provode kroz afinu transformaciju:
(c) kako bi se osiguralo da mreža ne gubi ekspresivnost

12. Tehnike regularizacije:
(c) povećavaju pristranost modela

13. Koja je vremenska složenost računanja
 izlaza u svim vremenskim koracima u 
dvosmjernoj višeslojnoj povratnoj neuronskoj mreži (bidirectional RNN),
 gdje je dubina mreže D, a broj vremenskih koraka odmatanja mreže T?
(b) T ∙ D


Evo odgovora na pitanja:

14. Koji je odnos između vektora a = softmax(x) i vektora b=softmax(x-max(x)):
(c) matematički, vrijedi a = b

15. Rano zaustavljanje ima regularizacijski efekt jer:
(c) osigurava ograničenu normu vektora parametara modela

16. Razmatramo k-ti sloj dubokog modela koji provodi transformaciju
 \( h_k = W_k \cdot h_{k-1} + b_k \). Zašto tijekom učenja treba pamtiti međurezultate \( h_k \):
(d) Računanje gradijenta po \( W_k \)

17. Može li korak gradijentnog spusta povećati gubitak mini-grupe:
(a) Da, ali samo ako je faktor pomaka prevelik

18. Razmatramo slučajnu varijablu \( Y = a \cdot x + b + W \), 
gdje \( x \) označava ulaz, a \( W \sim N(0, 100) \). 
Algoritam strojnog učenja koji uči vezu između \( Y \) i \( X \) polinomom 10. stupnja ima:
(a) Veliku varijancu, malu pristranost

19. Korištenje LSTM-a potpuno rješava:
(d) Problem nestajućeg gradijenta

20. Zašto u konvolucijskoj mreži koristimo sažimanje:
(c) Poboljšamo invarijantnost na pomake objekata i smanjimo dimenzionalnost izlaza

1. Koji izraz se koristi za ažuriranje skrivenog stanja povratne ćelije s dugoročnom memorijom (Long Short Term Memory)?
   - **c. 𝑐(𝑡) = 𝑓(𝑡) ∘ 𝑐(𝑡−1) + (𝑔(𝑡)) ∘ 𝑐̂(𝑡)**

2. Koju tehniku ne ubrajamo u regularizaciju?
   - **a. učenje sa zaletom**

3. Funkcija rotira točke ravnine za kut od 35 stupnjeva. Navedi dimenzije Jakobijeve matrice te funkcije.
   - **c. 2x2**

4. Zašto su konvolucijski slojevi ponekad prikladniji od potpuno povezanih slojeva?
   - **a. zbog regularizacijskog efekta usred dijeljenja parametara**

5. Za koji od parametara običnog povratnog modela postoji mogućnost eksplodirajućeg odnosno nestajućeg gradijenta?
   - **b. 𝑊ℎℎ**

6. Kolika je površina ispod krivulje preciznosti i
 odziva za binarnu klasifikaciju dvaju podataka ako su oznake Y=[0,1], a predikcije P(Y=1|x)=[0.9,0.8].
   - **b. 1.0**

7. Koja inačica kontrasnog gubitka je povezana s unakrsnom entropijom?
   - **a. gubitak N parova**

8. Razmatramo L2 regulariziranu funkciju gubitka dubokog modela
 tijekom provedbe jednog koraka stohastičkog gradijentnog spusta. 
negativni gradijent regularizacije pomiče model u smjeru:
   - **d. ishodišta prostora modela**

9. Razmatramo višerazrednu logističku regresiju s n značajki na ulazu.
 Ako prilikom učenja tog modela koristimo stohastičko izostavljanje značajki (droput), 
evaluacijom tako naučenog modela možemo dobiti:
   - **c. aritmetičku sredinu predikcije O(2^n) modela**

10. Koliko iznosi softmax([ln2, 0])?
    - **c. [2/3, 1/3]**

11. Za aktivacijsku funkciju latentnog sloja gotovo nikad nećemo koristiti:
    - **a. 𝑔(𝑥|𝛼, 𝛽) = 𝛽 + 𝛼 ∙ 𝑥**

12. Razmatramo računanje gradijenta funkcije cilja 
uzorkovanjem manjeg broja podataka (minigrupe) 
umjesto uporabe čitavog skupa uzoraka za učenje. Ta ideja je opravdana jer preciznost gradijenta:
    - **a. raste ispodlinearno s povećanjem broja uzoraka**


    Ovo su pitanja iz različitih područja strojnog učenja i dubokog učenja. Evo odgovora na svako pitanje:

1. **Kakav oblik ima 2D izlaz iz konvolucije trećeg reda oblika F×H×W, koristi se jezgra F×k×k i padding?**
   - Odgovor: b) F×H×W

2. **Polinom 10. stupnja uči funkciju ax + b + w. Kakvu varijancu i pristranost će imati?**
   - Ovo pitanje nije jasno formulirano, no pretpostavljamo da se misli na linearnu regresiju. U tom slučaju:
     - Varijanca: Ovisi o šumu u podacima i broju uzoraka.
     - Pristranost: Ovisi o odabiru uzoraka za učenje i modela.

3. **Koja je veza kvadratnog gubitka i negativne log izglednosti?**
   - Odgovor: d) Kvadratni gubitak je specijalni slučaj negativne log izglednosti.

4. **Ako imamo sliku 4x4 i dvaput uzastopno primijenimo 3x3 konvoluciju, dimenzije izlaza?**
   - Nakon prve konvolucije (s 3x3 kernelom):
     - Izlaz će biti 2x2 (jer je 4 - 3 + 1 = 2).
   - Nakon druge konvolucije (ponovno s 3x3 kernelom):
     - Izlaz će biti 1x1 (jer je 2 - 3 + 1 = 0).

5. **Koja procedura se ne koristi za linearno nerazdvojive podatke?**
   - Odgovor: c) Ručno oblikovano ugrađivanje.

6. **Prednost zglobnice nad log(1+exp(x))?**
   - Odgovor: c) Neprekinuta prva derivacija.

7. **Pri računanju gradijenata parametara težina u potpuno povezanom sloju, 
zbrajanje je najefikasnije provesti kojim pomagalom?**
   - Odgovor: b) Library za množenje matrica.

8. **Koji je nedostatak 0-1 gubitka?**
   - Nedostatak 0-1 gubitka je to što je on nelinearan
 i nema kontinuirane gradijente, što ga čini nepraktičnim za optimizaciju pomoću gradijentnog spusta.

9. **Kakve su konveksnosti gubitka logreg i dubokog modela?**
   - Gubitak logističke regresije (logreg) je konveksan jer je log-logistička funkcija gubitka konveksna.
   - Gubitak dubokog modela (kao što su gubitci kod neuronskih mreža)
 nije nužno konveksan jer modeli mogu imati mnogo lokalnih minimuma.

Ovo su odgovori na postavljena pitanja. Ako imate još nešto za pitati ili pojasniti, slobodno pitajte!