Evo odgovora na pitanja:

1. Prednost algoritma AdaGrad nad osnovnom implementacijom SGD-a je:
(b) skaliranje komponenata gradijenata njihovom akumuliranom normom

2. PoveÄ‡anjem veliÄine mini-grupe dobivamo:
(d) manji broj preciznijih aÅ¾uriranja parametara modela

3. ZaÅ¡to potpuno povezani modeli s prosjeÄnom unakrsnom entropijom imaju viÅ¡e jednako dobrih lokalnih minimuma?
(a) zbog simetrije neurona istog skrivenog sloja

4. Kako biste opisali odnos izmeÄ‘u gubitka â€0-1â€œ i unakrsne entropije?
(c) unakrsna entropija je derivabilni nadomjestak gubitka â€0-1â€œ

5. Kod uporabe lokalne normalizacije odziva (LRN), normaliziraju se:
(c) izlazi neurona s obzirom na izlaze njemu susjednih neurona u istom sloju

6. Predtreniranje je postupak:
(c) koji osigurava poÄetne vrijednosti parametara mreÅ¾e koje su bolje od sluÄajnih

7. Do problema eksplodirajuÄ‡eg gradijenta moÅ¾e doÄ‡i ako se podatci uzastopno
 mnoÅ¾e matricom teÅ¾ina za koju vrijedi:
(b) da ima neke svojstvene vrijednosti koje su po iznosu dosta veÄ‡e od 1

8. ZaÅ¡to pri optimizaciji radije promatramo log-izglednost nego izglednost?
(c) zbog aditivnosti gradijenata nezavisnih podataka

9. Uzorkovanje manjeg broja uzoraka (mini-grupe) umjesto uporabe
Äitavog skupa uzoraka za uÄenje pri izraÄunu gradijenata je opravdano jer:
(a) preciznost odreÄ‘ivanja gradijenta s poveÄ‡anjem broja uzoraka raste ispodlinearno

10. U kojoj od sljedeÄ‡ih situacija nam ne moÅ¾e pomoÄ‡i uÄenje s momentom?
(d) kod â€plohaâ€œ

11. Kod normalizacije nad grupom normalizirani se podatci na kraju provode kroz afinu transformaciju:
(c) kako bi se osiguralo da mreÅ¾a ne gubi ekspresivnost

12. Tehnike regularizacije:
(c) poveÄ‡avaju pristranost modela

13. Koja je vremenska sloÅ¾enost raÄunanja
 izlaza u svim vremenskim koracima u 
dvosmjernoj viÅ¡eslojnoj povratnoj neuronskoj mreÅ¾i (bidirectional RNN),
 gdje je dubina mreÅ¾e D, a broj vremenskih koraka odmatanja mreÅ¾e T?
(b) T âˆ™ D


Evo odgovora na pitanja:

14. Koji je odnos izmeÄ‘u vektora a = softmax(x) i vektora b=softmax(x-max(x)):
(c) matematiÄki, vrijedi a = b

15. Rano zaustavljanje ima regularizacijski efekt jer:
(c) osigurava ograniÄenu normu vektora parametara modela

16. Razmatramo k-ti sloj dubokog modela koji provodi transformaciju
 \( h_k = W_k \cdot h_{k-1} + b_k \). ZaÅ¡to tijekom uÄenja treba pamtiti meÄ‘urezultate \( h_k \):
(d) RaÄunanje gradijenta po \( W_k \)

17. MoÅ¾e li korak gradijentnog spusta poveÄ‡ati gubitak mini-grupe:
(a) Da, ali samo ako je faktor pomaka prevelik

18. Razmatramo sluÄajnu varijablu \( Y = a \cdot x + b + W \), 
gdje \( x \) oznaÄava ulaz, a \( W \sim N(0, 100) \). 
Algoritam strojnog uÄenja koji uÄi vezu izmeÄ‘u \( Y \) i \( X \) polinomom 10. stupnja ima:
(a) Veliku varijancu, malu pristranost

19. KoriÅ¡tenje LSTM-a potpuno rjeÅ¡ava:
(d) Problem nestajuÄ‡eg gradijenta

20. ZaÅ¡to u konvolucijskoj mreÅ¾i koristimo saÅ¾imanje:
(c) PoboljÅ¡amo invarijantnost na pomake objekata i smanjimo dimenzionalnost izlaza

1. Koji izraz se koristi za aÅ¾uriranje skrivenog stanja povratne Ä‡elije s dugoroÄnom memorijom (Long Short Term Memory)?
   - **c. ğ‘(ğ‘¡) = ğ‘“(ğ‘¡) âˆ˜ ğ‘(ğ‘¡âˆ’1) + (ğ‘”(ğ‘¡)) âˆ˜ ğ‘Ì‚(ğ‘¡)**

2. Koju tehniku ne ubrajamo u regularizaciju?
   - **a. uÄenje sa zaletom**

3. Funkcija rotira toÄke ravnine za kut od 35 stupnjeva. Navedi dimenzije Jakobijeve matrice te funkcije.
   - **c. 2x2**

4. ZaÅ¡to su konvolucijski slojevi ponekad prikladniji od potpuno povezanih slojeva?
   - **a. zbog regularizacijskog efekta usred dijeljenja parametara**

5. Za koji od parametara obiÄnog povratnog modela postoji moguÄ‡nost eksplodirajuÄ‡eg odnosno nestajuÄ‡eg gradijenta?
   - **b. ğ‘Šâ„â„**

6. Kolika je povrÅ¡ina ispod krivulje preciznosti i
 odziva za binarnu klasifikaciju dvaju podataka ako su oznake Y=[0,1], a predikcije P(Y=1|x)=[0.9,0.8].
   - **b. 1.0**

7. Koja inaÄica kontrasnog gubitka je povezana s unakrsnom entropijom?
   - **a. gubitak N parova**

8. Razmatramo L2 regulariziranu funkciju gubitka dubokog modela
 tijekom provedbe jednog koraka stohastiÄkog gradijentnog spusta. 
negativni gradijent regularizacije pomiÄe model u smjeru:
   - **d. ishodiÅ¡ta prostora modela**

9. Razmatramo viÅ¡erazrednu logistiÄku regresiju s n znaÄajki na ulazu.
 Ako prilikom uÄenja tog modela koristimo stohastiÄko izostavljanje znaÄajki (droput), 
evaluacijom tako nauÄenog modela moÅ¾emo dobiti:
   - **c. aritmetiÄku sredinu predikcije O(2^n) modela**

10. Koliko iznosi softmax([ln2, 0])?
    - **c. [2/3, 1/3]**

11. Za aktivacijsku funkciju latentnog sloja gotovo nikad neÄ‡emo koristiti:
    - **a. ğ‘”(ğ‘¥|ğ›¼, ğ›½) = ğ›½ + ğ›¼ âˆ™ ğ‘¥**

12. Razmatramo raÄunanje gradijenta funkcije cilja 
uzorkovanjem manjeg broja podataka (minigrupe) 
umjesto uporabe Äitavog skupa uzoraka za uÄenje. Ta ideja je opravdana jer preciznost gradijenta:
    - **a. raste ispodlinearno s poveÄ‡anjem broja uzoraka**


    Ovo su pitanja iz razliÄitih podruÄja strojnog uÄenja i dubokog uÄenja. Evo odgovora na svako pitanje:

1. **Kakav oblik ima 2D izlaz iz konvolucije treÄ‡eg reda oblika FÃ—HÃ—W, koristi se jezgra FÃ—kÃ—k i padding?**
   - Odgovor: b) FÃ—HÃ—W

2. **Polinom 10. stupnja uÄi funkciju ax + b + w. Kakvu varijancu i pristranost Ä‡e imati?**
   - Ovo pitanje nije jasno formulirano, no pretpostavljamo da se misli na linearnu regresiju. U tom sluÄaju:
     - Varijanca: Ovisi o Å¡umu u podacima i broju uzoraka.
     - Pristranost: Ovisi o odabiru uzoraka za uÄenje i modela.

3. **Koja je veza kvadratnog gubitka i negativne log izglednosti?**
   - Odgovor: d) Kvadratni gubitak je specijalni sluÄaj negativne log izglednosti.

4. **Ako imamo sliku 4x4 i dvaput uzastopno primijenimo 3x3 konvoluciju, dimenzije izlaza?**
   - Nakon prve konvolucije (s 3x3 kernelom):
     - Izlaz Ä‡e biti 2x2 (jer je 4 - 3 + 1 = 2).
   - Nakon druge konvolucije (ponovno s 3x3 kernelom):
     - Izlaz Ä‡e biti 1x1 (jer je 2 - 3 + 1 = 0).

5. **Koja procedura se ne koristi za linearno nerazdvojive podatke?**
   - Odgovor: c) RuÄno oblikovano ugraÄ‘ivanje.

6. **Prednost zglobnice nad log(1+exp(x))?**
   - Odgovor: c) Neprekinuta prva derivacija.

7. **Pri raÄunanju gradijenata parametara teÅ¾ina u potpuno povezanom sloju, 
zbrajanje je najefikasnije provesti kojim pomagalom?**
   - Odgovor: b) Library za mnoÅ¾enje matrica.

8. **Koji je nedostatak 0-1 gubitka?**
   - Nedostatak 0-1 gubitka je to Å¡to je on nelinearan
 i nema kontinuirane gradijente, Å¡to ga Äini nepraktiÄnim za optimizaciju pomoÄ‡u gradijentnog spusta.

9. **Kakve su konveksnosti gubitka logreg i dubokog modela?**
   - Gubitak logistiÄke regresije (logreg) je konveksan jer je log-logistiÄka funkcija gubitka konveksna.
   - Gubitak dubokog modela (kao Å¡to su gubitci kod neuronskih mreÅ¾a)
 nije nuÅ¾no konveksan jer modeli mogu imati mnogo lokalnih minimuma.

Ovo su odgovori na postavljena pitanja. Ako imate joÅ¡ neÅ¡to za pitati ili pojasniti, slobodno pitajte!